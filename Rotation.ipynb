{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Rotation.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOReF7FkBh30bnLS91Mphbb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tqphu27/Rotation_MC-OCR/blob/main/Rotation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jj-HxGasH-3p",
        "outputId": "d95a1897-aa84-4bf0-9c18-3ee1451dcc95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/gdrive/MyDrive/image_to_rotate.zip"
      ],
      "metadata": {
        "id": "WszOIaCcYytZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78135da9-2883-41fb-9be1-2b8f7c72ee0e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/gdrive/MyDrive/image_to_rotate.zip\n",
            "   creating: image_to_rotate/\n",
            "  inflating: image_to_rotate/mcocr_public_145013joykg.png  \n",
            "  inflating: image_to_rotate/mcocr_public_145014cmdbw.png  \n",
            "  inflating: image_to_rotate/mcocr_public_145014hxoeu.png  \n",
            "  inflating: image_to_rotate/mcocr_public_145014kxqtn.png  \n",
            "  inflating: image_to_rotate/mcocr_public_145014ancat.png  \n",
            "  inflating: image_to_rotate/mcocr_public_145013geoih.png  \n",
            "  inflating: image_to_rotate/mcocr_public_145013kgypr.png  \n",
            "  inflating: image_to_rotate/mcocr_public_145014nsrnp.png  \n",
            "  inflating: image_to_rotate/mcocr_public_145013kgaqa.png  \n",
            "  inflating: image_to_rotate/mcocr_public_145013awcbh.png  \n",
            "  inflating: image_to_rotate/mcocr_public_145014hdquz.png  \n",
            "  inflating: image_to_rotate/mcocr_val_145115ndmhc.png  \n",
            "  inflating: image_to_rotate/mcocr_public_145014vrbsr.png  \n",
            "  inflating: image_to_rotate/mcocr_public_145013avtwc.png  \n",
            "  inflating: image_to_rotate/mcocr_public_145014kyxew.png  \n",
            "  inflating: image_to_rotate/mcocr_public_145014iueod.png  \n",
            "  inflating: image_to_rotate/mcocr_val_145115mxzac.png  \n",
            "  inflating: image_to_rotate/mcocr_public_145013ahfqj.png  \n",
            "  inflating: image_to_rotate/mcocr_public_145014bokrq.png  \n",
            "  inflating: image_to_rotate/mcocr_public_145013lkorp.png  \n",
            "  inflating: image_to_rotate/mcocr_public_145013lmhud.png  \n",
            "  inflating: image_to_rotate/mcocr_public_145013fiibr.png  \n",
            "  inflating: image_to_rotate/mcocr_public_145014zsdex.png  \n",
            "  inflating: image_to_rotate/mcocr_public_145013clltn.png  \n",
            "  inflating: image_to_rotate/mcocr_public_145014vxrob.png  \n",
            "  inflating: image_to_rotate/mcocr_val_145114lbdpw.png  \n",
            "  inflating: image_to_rotate/mcocr_public_145013cnknh.png  \n",
            "  inflating: image_to_rotate/mcocr_public_145014mprun.png  \n",
            "  inflating: image_to_rotate/mcocr_public_145013hfxkr.png  \n",
            "  inflating: image_to_rotate/mcocr_public_145014dplaq.png  \n",
            "  inflating: image_to_rotate/mcocr_public_145013ilier.png  \n",
            "  inflating: image_to_rotate/mcocr_public_145014mthgu.png  \n",
            "  inflating: image_to_rotate/mcocr_val_145115cfwch.png  \n",
            "  inflating: image_to_rotate/mcocr_public_145014wxzpz.png  \n",
            "  inflating: image_to_rotate/mcocr_val_145115itmlf.png  \n",
            "  inflating: image_to_rotate/mcocr_public_145013apigo.png  \n",
            "  inflating: image_to_rotate/mcocr_public_145013otehh.png  \n",
            "  inflating: image_to_rotate/mcocr_public_145013djwis.png  \n",
            "  inflating: image_to_rotate/mcocr_val_145115rltwx.png  \n",
            "  inflating: image_to_rotate/mcocr_public_145014jkafe.png  \n",
            "  inflating: image_to_rotate/mcocr_public_145013haeps.png  \n",
            "  inflating: image_to_rotate/mcocr_public_145014jkegi.png  \n",
            "  inflating: image_to_rotate/mcocr_public_145014ckynq.png  \n",
            "  inflating: image_to_rotate/mcocr_public_145013njeuf.png  \n",
            "  inflating: image_to_rotate/mcocr_val_145115fkifm.png  \n",
            "  inflating: image_to_rotate/mcocr_public_145014kvcfd.png  \n",
            "  inflating: image_to_rotate/mcocr_public_145014jxscx.png  \n",
            "  inflating: image_to_rotate/mcocr_val_145115obsjf.png  \n",
            "  inflating: image_to_rotate/mcocr_public_145013iblsz.png  \n",
            "  inflating: image_to_rotate/mcocr_public_145014gvuhh.png  \n",
            "  inflating: image_to_rotate/mcocr_public_145014nukhy.png  \n",
            "  inflating: image_to_rotate/mcocr_public_145014ksyep.png  \n",
            "  inflating: image_to_rotate/mcocr_public_145014jgtbj.png  \n",
            "  inflating: image_to_rotate/mcocr_public_145014fxqlw.png  \n",
            "  inflating: image_to_rotate/mcocr_public_145014hdevu.png  \n",
            "  inflating: image_to_rotate/mcocr_public_145014hcbdo.png  \n",
            "  inflating: image_to_rotate/mcocr_public_145014xooen.png  \n",
            "  inflating: image_to_rotate/mcocr_public_145014anjdg.png  \n",
            "  inflating: image_to_rotate/mcocr_public_145014avkgm.png  \n",
            "  inflating: image_to_rotate/mcocr_public_145013qvqeu.png  \n",
            "  inflating: image_to_rotate/mcocr_public_145014tbhtz.png  \n",
            "  inflating: image_to_rotate/mcocr_public_145013imsqw.png  \n",
            "  inflating: image_to_rotate/mcocr_val_145114ixmyt.png  \n",
            "  inflating: image_to_rotate/mcocr_public_145013qikxs.png  \n",
            "  inflating: image_to_rotate/mcocr_val_145115ujvia.png  \n",
            "  inflating: image_to_rotate/mcocr_public_145014dhito.png  \n",
            "  inflating: image_to_rotate/mcocr_public_145013bcovr.png  \n",
            "  inflating: image_to_rotate/mcocr_public_145014ehtue.png  \n",
            "  inflating: image_to_rotate/mcocr_public_145014yhgfq.png  \n",
            "  inflating: image_to_rotate/mcocr_public_145014scxqk.png  \n",
            "  inflating: image_to_rotate/mcocr_public_145014motth.png  \n",
            "  inflating: image_to_rotate/mcocr_public_145014lbzhz.png  \n",
            "  inflating: image_to_rotate/mcocr_public_145013bdnvs.png  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Levenshtein\n",
        "!pip install einops"
      ],
      "metadata": {
        "id": "X1FY4hhTLE1m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c52a6d4-52b7-4617-a022-a2ae7e03b2a4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Levenshtein\n",
            "  Downloading Levenshtein-0.18.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (258 kB)\n",
            "\u001b[K     |████████████████████████████████| 258 kB 5.3 MB/s \n",
            "\u001b[?25hCollecting rapidfuzz<3.0.0,>=2.0.1\n",
            "  Downloading rapidfuzz-2.0.11-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 46.5 MB/s \n",
            "\u001b[?25hCollecting jarowinkler<1.1.0,>=1.0.2\n",
            "  Downloading jarowinkler-1.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (103 kB)\n",
            "\u001b[K     |████████████████████████████████| 103 kB 49.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: jarowinkler, rapidfuzz, Levenshtein\n",
            "Successfully installed Levenshtein-0.18.1 jarowinkler-1.0.2 rapidfuzz-2.0.11\n",
            "Collecting einops\n",
            "  Downloading einops-0.4.1-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: einops\n",
            "Successfully installed einops-0.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gdrive/MyDrive/MC_OCR/mc_ocr/rotation_corrector\n",
        "!python3 inference.py"
      ],
      "metadata": {
        "id": "ZhGOADvkIAXq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27532723-818e-4f89-d4ee-6a68561e217f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/MC_OCR/mc_ocr/rotation_corrector\n",
            "Traceback (most recent call last):\n",
            "  File \"inference.py\", line 9, in <module>\n",
            "    from mc_ocr.rotation_corrector.predict import init_box_rectify_model\n",
            "  File \"/content/gdrive/MyDrive/MC_OCR/mc_ocr/rotation_corrector/predict.py\", line 9, in <module>\n",
            "    import torch\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/__init__.py\", line 197, in <module>\n",
            "    from torch._C import *  # noqa: F403\n",
            "RuntimeError: KeyboardInterrupt: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/gdrive/MyDrive/MC_OCR2/mc_ocr/text_classifier /content/gdrive/MyDrive/MC_OCR/mc_ocr"
      ],
      "metadata": {
        "id": "w2eLThQxa4qs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gdrive/MyDrive/MC_OCR/mc_ocr/rotation_corrector\n",
        "!python3 process_mc_ocr_data.py"
      ],
      "metadata": {
        "id": "25aqDDli1qq-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2866264e-cf66-4e86-b747-445d05af0fb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/MC_OCR/mc_ocr/rotation_corrector\n",
            "Use GPU 0\n",
            "Classifier_Vietocr. Init\n",
            "Init models time: 5.159385681152344 seconds\n",
            "\n",
            " 0 Inference mcocr_public_145013ahfqj.png\n",
            "get boxes from icdar time: 0.005715847015380859 seconds\n",
            "Classifier_Vietocr. Inference 29 boxes\n",
            "mean prob: 0.7490023545041121\n",
            "Multiscale OCR time: 0.0 seconds\n",
            "Total predict time: 1.0303266048431396 seconds\n",
            "Save visualized result to /content/gdrive/MyDrive/MC_OCR/Data2/viz_imgs/mcocr_public_145013ahfqj.png\n",
            "findfont: Font family ['TakaoPGothic'] not found. Falling back to DejaVu Sans.\n",
            "findfont: Font family ['TakaoPGothic'] not found. Falling back to DejaVu Sans.\n",
            "Visualize time: 2.0348024368286133 seconds\n",
            "\n",
            " 1 Inference mcocr_public_145013apigo.png\n",
            "get boxes from icdar time: 0.001956939697265625 seconds\n",
            "Classifier_Vietocr. Inference 42 boxes\n",
            "mean prob: 0.7263566001151448\n",
            "Multiscale OCR time: 0.0 seconds\n",
            "Total predict time: 1.8758318424224854 seconds\n",
            "Save visualized result to /content/gdrive/MyDrive/MC_OCR/Data2/viz_imgs/mcocr_public_145013apigo.png\n",
            "Visualize time: 1.1148481369018555 seconds\n",
            "\n",
            " 2 Inference mcocr_public_145013avtwc.png\n",
            "get boxes from icdar time: 0.0026390552520751953 seconds\n",
            "Classifier_Vietocr. Inference 73 boxes\n",
            "mean prob: 0.688024518180052\n",
            "\n",
            " 3 Inference mcocr_public_145013awcbh.png\n",
            "get boxes from icdar time: 0.0021817684173583984 seconds\n",
            "Classifier_Vietocr. Inference 26 boxes\n",
            "mean prob: 0.6187409352996609\n",
            "\n",
            " 4 Inference mcocr_public_145013bcovr.png\n",
            "get boxes from icdar time: 0.0022788047790527344 seconds\n",
            "Classifier_Vietocr. Inference 35 boxes\n",
            "mean prob: 0.6889130829495008\n",
            "\n",
            " 5 Inference mcocr_public_145013bdnvs.png\n",
            "get boxes from icdar time: 0.001981019973754883 seconds\n",
            "Classifier_Vietocr. Inference 29 boxes\n",
            "mean prob: 0.64543031312761\n",
            "\n",
            " 6 Inference mcocr_public_145013clltn.png\n",
            "get boxes from icdar time: 0.0022852420806884766 seconds\n",
            "Classifier_Vietocr. Inference 43 boxes\n",
            "mean prob: 0.6635400183249259\n",
            "\n",
            " 7 Inference mcocr_public_145013cnknh.png\n",
            "get boxes from icdar time: 0.0022504329681396484 seconds\n",
            "Classifier_Vietocr. Inference 29 boxes\n",
            "mean prob: 0.6479895442311748\n",
            "\n",
            " 8 Inference mcocr_public_145013djwis.png\n",
            "get boxes from icdar time: 0.002155303955078125 seconds\n",
            "Classifier_Vietocr. Inference 27 boxes\n",
            "mean prob: 0.7208766095056149\n",
            "Multiscale OCR time: 0.0 seconds\n",
            "Total predict time: 0.9547147750854492 seconds\n",
            "Save visualized result to /content/gdrive/MyDrive/MC_OCR/Data2/viz_imgs/mcocr_public_145013djwis.png\n",
            "Visualize time: 1.432715654373169 seconds\n",
            "\n",
            " 9 Inference mcocr_public_145013fiibr.png\n",
            "get boxes from icdar time: 0.0023682117462158203 seconds\n",
            "Classifier_Vietocr. Inference 31 boxes\n",
            "mean prob: 0.7114941062632656\n",
            "Multiscale OCR time: 0.0 seconds\n",
            "Total predict time: 1.0523691177368164 seconds\n",
            "Save visualized result to /content/gdrive/MyDrive/MC_OCR/Data2/viz_imgs/mcocr_public_145013fiibr.png\n",
            "Visualize time: 1.6070196628570557 seconds\n",
            "\n",
            " 10 Inference mcocr_public_145013geoih.png\n",
            "get boxes from icdar time: 0.0021293163299560547 seconds\n",
            "Classifier_Vietocr. Inference 20 boxes\n",
            "mean prob: 0.6103705080796262\n",
            "\n",
            " 11 Inference mcocr_public_145013haeps.png\n",
            "get boxes from icdar time: 0.002031564712524414 seconds\n",
            "Classifier_Vietocr. Inference 22 boxes\n",
            "mean prob: 0.7257750417567169\n",
            "Multiscale OCR time: 0.0 seconds\n",
            "Total predict time: 1.0341899394989014 seconds\n",
            "Save visualized result to /content/gdrive/MyDrive/MC_OCR/Data2/viz_imgs/mcocr_public_145013haeps.png\n",
            "Visualize time: 1.7246677875518799 seconds\n",
            "\n",
            " 12 Inference mcocr_public_145013hfxkr.png\n",
            "get boxes from icdar time: 0.0022735595703125 seconds\n",
            "Classifier_Vietocr. Inference 54 boxes\n",
            "mean prob: 0.6641701248446774\n",
            "\n",
            " 13 Inference mcocr_public_145013iblsz.png\n",
            "get boxes from icdar time: 0.0023279190063476562 seconds\n",
            "Classifier_Vietocr. Inference 25 boxes\n",
            "mean prob: 0.6860414778756713\n",
            "\n",
            " 14 Inference mcocr_public_145013ilier.png\n",
            "get boxes from icdar time: 0.0020220279693603516 seconds\n",
            "Classifier_Vietocr. Inference 28 boxes\n",
            "mean prob: 0.6978833145802351\n",
            "\n",
            " 15 Inference mcocr_public_145013imsqw.png\n",
            "get boxes from icdar time: 0.002312898635864258 seconds\n",
            "Classifier_Vietocr. Inference 52 boxes\n",
            "mean prob: 0.6955771342261333\n",
            "\n",
            " 16 Inference mcocr_public_145013joykg.png\n",
            "get boxes from icdar time: 0.0020084381103515625 seconds\n",
            "Classifier_Vietocr. Inference 19 boxes\n",
            "mean prob: 0.5968427308284313\n",
            "\n",
            " 17 Inference mcocr_public_145013kgaqa.png\n",
            "get boxes from icdar time: 0.0023331642150878906 seconds\n",
            "Classifier_Vietocr. Inference 38 boxes\n",
            "mean prob: 0.6617419229193061\n",
            "\n",
            " 18 Inference mcocr_public_145013kgypr.png\n",
            "get boxes from icdar time: 0.0022745132446289062 seconds\n",
            "Classifier_Vietocr. Inference 36 boxes\n",
            "mean prob: 0.6976116467013821\n",
            "\n",
            " 19 Inference mcocr_public_145013lkorp.png\n",
            "get boxes from icdar time: 0.0025467872619628906 seconds\n",
            "Classifier_Vietocr. Inference 28 boxes\n",
            "mean prob: 0.6875297093468934\n",
            "\n",
            " 20 Inference mcocr_public_145013lmhud.png\n",
            "get boxes from icdar time: 0.002167940139770508 seconds\n",
            "Classifier_Vietocr. Inference 14 boxes\n",
            "mean prob: 0.6854471848598844\n",
            "\n",
            " 21 Inference mcocr_public_145013njeuf.png\n",
            "get boxes from icdar time: 0.002104043960571289 seconds\n",
            "Classifier_Vietocr. Inference 34 boxes\n",
            "mean prob: 0.7364887223354897\n",
            "Multiscale OCR time: 0.0 seconds\n",
            "Total predict time: 1.0961809158325195 seconds\n",
            "Save visualized result to /content/gdrive/MyDrive/MC_OCR/Data2/viz_imgs/mcocr_public_145013njeuf.png\n",
            "Visualize time: 1.5965561866760254 seconds\n",
            "\n",
            " 22 Inference mcocr_public_145013otehh.png\n",
            "get boxes from icdar time: 0.002969503402709961 seconds\n",
            "Classifier_Vietocr. Inference 34 boxes\n",
            "mean prob: 0.5982234419756923\n",
            "\n",
            " 23 Inference mcocr_public_145013qikxs.png\n",
            "get boxes from icdar time: 0.0021483898162841797 seconds\n",
            "Classifier_Vietocr. Inference 23 boxes\n",
            "mean prob: 0.6783594776067197\n",
            "\n",
            " 24 Inference mcocr_public_145013qvqeu.png\n",
            "get boxes from icdar time: 0.002164602279663086 seconds\n",
            "Classifier_Vietocr. Inference 34 boxes\n",
            "mean prob: 0.6643769578251137\n",
            "\n",
            " 25 Inference mcocr_public_145014ancat.png\n",
            "get boxes from icdar time: 0.0030062198638916016 seconds\n",
            "Classifier_Vietocr. Inference 40 boxes\n",
            "mean prob: 0.748318358175403\n",
            "Multiscale OCR time: 0.0 seconds\n",
            "Total predict time: 1.303408145904541 seconds\n",
            "Save visualized result to /content/gdrive/MyDrive/MC_OCR/Data2/viz_imgs/mcocr_public_145014ancat.png\n",
            "Visualize time: 1.5662012100219727 seconds\n",
            "\n",
            " 26 Inference mcocr_public_145014anjdg.png\n",
            "get boxes from icdar time: 0.0021774768829345703 seconds\n",
            "Classifier_Vietocr. Inference 34 boxes\n",
            "mean prob: 0.6432584780071301\n",
            "\n",
            " 27 Inference mcocr_public_145014avkgm.png\n",
            "get boxes from icdar time: 0.002350330352783203 seconds\n",
            "Classifier_Vietocr. Inference 33 boxes\n",
            "mean prob: 0.6901280550099518\n",
            "\n",
            " 28 Inference mcocr_public_145014bokrq.png\n",
            "get boxes from icdar time: 0.002200603485107422 seconds\n",
            "Classifier_Vietocr. Inference 38 boxes\n",
            "mean prob: 0.627010901403559\n",
            "\n",
            " 29 Inference mcocr_public_145014ckynq.png\n",
            "get boxes from icdar time: 0.002492666244506836 seconds\n",
            "Classifier_Vietocr. Inference 44 boxes\n",
            "mean prob: 0.7403124852940732\n",
            "Multiscale OCR time: 0.0 seconds\n",
            "Total predict time: 1.5665102005004883 seconds\n",
            "Save visualized result to /content/gdrive/MyDrive/MC_OCR/Data2/viz_imgs/mcocr_public_145014ckynq.png\n",
            "Visualize time: 1.6409735679626465 seconds\n",
            "\n",
            " 30 Inference mcocr_public_145014cmdbw.png\n",
            "get boxes from icdar time: 0.0021924972534179688 seconds\n",
            "Classifier_Vietocr. Inference 39 boxes\n",
            "mean prob: 0.6586181104472759\n",
            "\n",
            " 31 Inference mcocr_public_145014dhito.png\n",
            "get boxes from icdar time: 0.0024051666259765625 seconds\n",
            "Classifier_Vietocr. Inference 40 boxes\n",
            "mean prob: 0.6317882099003157\n",
            "\n",
            " 32 Inference mcocr_public_145014dplaq.png\n",
            "get boxes from icdar time: 0.002538442611694336 seconds\n",
            "Classifier_Vietocr. Inference 31 boxes\n",
            "mean prob: 0.6624014837974717\n",
            "\n",
            " 33 Inference mcocr_public_145014ehtue.png\n",
            "get boxes from icdar time: 0.002307415008544922 seconds\n",
            "Classifier_Vietocr. Inference 61 boxes\n",
            "mean prob: 0.6562958216101442\n",
            "\n",
            " 34 Inference mcocr_public_145014fxqlw.png\n",
            "get boxes from icdar time: 0.0024039745330810547 seconds\n",
            "Classifier_Vietocr. Inference 56 boxes\n",
            "mean prob: 0.6007678675902998\n",
            "\n",
            " 35 Inference mcocr_public_145014gvuhh.png\n",
            "get boxes from icdar time: 0.0031502246856689453 seconds\n",
            "Classifier_Vietocr. Inference 25 boxes\n",
            "mean prob: 0.7520640486025592\n",
            "Multiscale OCR time: 0.0 seconds\n",
            "Total predict time: 0.9343733787536621 seconds\n",
            "Save visualized result to /content/gdrive/MyDrive/MC_OCR/Data2/viz_imgs/mcocr_public_145014gvuhh.png\n",
            "Visualize time: 1.5714035034179688 seconds\n",
            "\n",
            " 36 Inference mcocr_public_145014hcbdo.png\n",
            "get boxes from icdar time: 0.0020668506622314453 seconds\n",
            "Classifier_Vietocr. Inference 35 boxes\n",
            "mean prob: 0.708868073695039\n",
            "Multiscale OCR time: 0.0 seconds\n",
            "Total predict time: 1.5841073989868164 seconds\n",
            "Save visualized result to /content/gdrive/MyDrive/MC_OCR/Data2/viz_imgs/mcocr_public_145014hcbdo.png\n",
            "Visualize time: 1.6323347091674805 seconds\n",
            "\n",
            " 37 Inference mcocr_public_145014hdevu.png\n",
            "get boxes from icdar time: 0.0023374557495117188 seconds\n",
            "Classifier_Vietocr. Inference 27 boxes\n",
            "mean prob: 0.8010930857764832\n",
            "Multiscale OCR time: 0.0 seconds\n",
            "Total predict time: 1.0654051303863525 seconds\n",
            "Save visualized result to /content/gdrive/MyDrive/MC_OCR/Data2/viz_imgs/mcocr_public_145014hdevu.png\n",
            "Visualize time: 1.5613629817962646 seconds\n",
            "\n",
            " 38 Inference mcocr_public_145014hdquz.png\n",
            "get boxes from icdar time: 0.0023212432861328125 seconds\n",
            "Classifier_Vietocr. Inference 36 boxes\n",
            "mean prob: 0.6503572996940964\n",
            "\n",
            " 39 Inference mcocr_public_145014hxoeu.png\n",
            "get boxes from icdar time: 0.0022306442260742188 seconds\n",
            "Classifier_Vietocr. Inference 25 boxes\n",
            "mean prob: 0.6896773273255182\n",
            "\n",
            " 40 Inference mcocr_public_145014iueod.png\n",
            "get boxes from icdar time: 0.0021407604217529297 seconds\n",
            "Classifier_Vietocr. Inference 51 boxes\n",
            "mean prob: 0.6552027654720299\n",
            "\n",
            " 41 Inference mcocr_public_145014jgtbj.png\n",
            "get boxes from icdar time: 0.001953125 seconds\n",
            "Classifier_Vietocr. Inference 30 boxes\n",
            "mean prob: 0.7343614606577114\n",
            "Multiscale OCR time: 0.0 seconds\n",
            "Total predict time: 1.3209328651428223 seconds\n",
            "Save visualized result to /content/gdrive/MyDrive/MC_OCR/Data2/viz_imgs/mcocr_public_145014jgtbj.png\n",
            "Visualize time: 1.5636990070343018 seconds\n",
            "\n",
            " 42 Inference mcocr_public_145014jkafe.png\n",
            "get boxes from icdar time: 0.0024683475494384766 seconds\n",
            "Classifier_Vietocr. Inference 31 boxes\n",
            "mean prob: 0.7185111076270486\n",
            "Multiscale OCR time: 0.0 seconds\n",
            "Total predict time: 1.340470790863037 seconds\n",
            "Save visualized result to /content/gdrive/MyDrive/MC_OCR/Data2/viz_imgs/mcocr_public_145014jkafe.png\n",
            "Visualize time: 1.627323865890503 seconds\n",
            "\n",
            " 43 Inference mcocr_public_145014jkegi.png\n",
            "get boxes from icdar time: 0.002142190933227539 seconds\n",
            "Classifier_Vietocr. Inference 17 boxes\n",
            "mean prob: 0.6325764430952897\n",
            "\n",
            " 44 Inference mcocr_public_145014jxscx.png\n",
            "get boxes from icdar time: 0.002349853515625 seconds\n",
            "Classifier_Vietocr. Inference 35 boxes\n",
            "mean prob: 0.642117253178342\n",
            "\n",
            " 45 Inference mcocr_public_145014ksyep.png\n",
            "get boxes from icdar time: 0.0024802684783935547 seconds\n",
            "Classifier_Vietocr. Inference 40 boxes\n",
            "mean prob: 0.6164879737114408\n",
            "\n",
            " 46 Inference mcocr_public_145014kvcfd.png\n",
            "get boxes from icdar time: 0.0020761489868164062 seconds\n",
            "Classifier_Vietocr. Inference 40 boxes\n",
            "mean prob: 0.668076065203594\n",
            "\n",
            " 47 Inference mcocr_public_145014kxqtn.png\n",
            "get boxes from icdar time: 0.00206756591796875 seconds\n",
            "Classifier_Vietocr. Inference 34 boxes\n",
            "mean prob: 0.6448631562872515\n",
            "\n",
            " 48 Inference mcocr_public_145014kyxew.png\n",
            "get boxes from icdar time: 0.002351045608520508 seconds\n",
            "Classifier_Vietocr. Inference 32 boxes\n",
            "mean prob: 0.6609267711983836\n",
            "\n",
            " 49 Inference mcocr_public_145014lbzhz.png\n",
            "get boxes from icdar time: 0.0021903514862060547 seconds\n",
            "Classifier_Vietocr. Inference 14 boxes\n",
            "mean prob: 0.6847000107976253\n",
            "\n",
            " 50 Inference mcocr_public_145014motth.png\n",
            "get boxes from icdar time: 0.002240419387817383 seconds\n",
            "Classifier_Vietocr. Inference 28 boxes\n",
            "mean prob: 0.6812612632993398\n",
            "\n",
            " 51 Inference mcocr_public_145014mprun.png\n",
            "get boxes from icdar time: 0.0022950172424316406 seconds\n",
            "Classifier_Vietocr. Inference 23 boxes\n",
            "mean prob: 0.6685875571523058\n",
            "\n",
            " 52 Inference mcocr_public_145014mthgu.png\n",
            "get boxes from icdar time: 0.002246856689453125 seconds\n",
            "Classifier_Vietocr. Inference 33 boxes\n",
            "mean prob: 0.6126948308500777\n",
            "\n",
            " 53 Inference mcocr_public_145014nsrnp.png\n",
            "get boxes from icdar time: 0.0023360252380371094 seconds\n",
            "Classifier_Vietocr. Inference 29 boxes\n",
            "mean prob: 0.5108984055356405\n",
            "\n",
            " 54 Inference mcocr_public_145014nukhy.png\n",
            "get boxes from icdar time: 0.001909494400024414 seconds\n",
            "Classifier_Vietocr. Inference 18 boxes\n",
            "mean prob: 0.6664264593171536\n",
            "\n",
            " 55 Inference mcocr_public_145014scxqk.png\n",
            "get boxes from icdar time: 0.0018553733825683594 seconds\n",
            "Classifier_Vietocr. Inference 15 boxes\n",
            "mean prob: 0.6318650048500651\n",
            "\n",
            " 56 Inference mcocr_public_145014tbhtz.png\n",
            "get boxes from icdar time: 0.002274036407470703 seconds\n",
            "Classifier_Vietocr. Inference 30 boxes\n",
            "mean prob: 0.7872679173362112\n",
            "Multiscale OCR time: 0.0 seconds\n",
            "Total predict time: 1.0281760692596436 seconds\n",
            "Save visualized result to /content/gdrive/MyDrive/MC_OCR/Data2/viz_imgs/mcocr_public_145014tbhtz.png\n",
            "Visualize time: 1.6517493724822998 seconds\n",
            "\n",
            " 57 Inference mcocr_public_145014vrbsr.png\n",
            "get boxes from icdar time: 0.002351999282836914 seconds\n",
            "Classifier_Vietocr. Inference 38 boxes\n",
            "mean prob: 0.6698511922116193\n",
            "\n",
            " 58 Inference mcocr_public_145014vxrob.png\n",
            "get boxes from icdar time: 0.002554655075073242 seconds\n",
            "Classifier_Vietocr. Inference 23 boxes\n",
            "mean prob: 0.7404114566619103\n",
            "Multiscale OCR time: 0.0 seconds\n",
            "Total predict time: 0.922579288482666 seconds\n",
            "Save visualized result to /content/gdrive/MyDrive/MC_OCR/Data2/viz_imgs/mcocr_public_145014vxrob.png\n",
            "Visualize time: 1.3905930519104004 seconds\n",
            "\n",
            " 59 Inference mcocr_public_145014wxzpz.png\n",
            "get boxes from icdar time: 0.0031414031982421875 seconds\n",
            "Classifier_Vietocr. Inference 28 boxes\n",
            "mean prob: 0.664024355315188\n",
            "\n",
            " 60 Inference mcocr_public_145014xooen.png\n",
            "get boxes from icdar time: 0.0022974014282226562 seconds\n",
            "Classifier_Vietocr. Inference 24 boxes\n",
            "mean prob: 0.7297954752962847\n",
            "Multiscale OCR time: 0.0 seconds\n",
            "Total predict time: 0.9115462303161621 seconds\n",
            "Save visualized result to /content/gdrive/MyDrive/MC_OCR/Data2/viz_imgs/mcocr_public_145014xooen.png\n",
            "Visualize time: 1.547071933746338 seconds\n",
            "\n",
            " 61 Inference mcocr_public_145014yhgfq.png\n",
            "get boxes from icdar time: 0.0023834705352783203 seconds\n",
            "Classifier_Vietocr. Inference 30 boxes\n",
            "mean prob: 0.7259699271808208\n",
            "Multiscale OCR time: 0.0 seconds\n",
            "Total predict time: 1.0363891124725342 seconds\n",
            "Save visualized result to /content/gdrive/MyDrive/MC_OCR/Data2/viz_imgs/mcocr_public_145014yhgfq.png\n",
            "Visualize time: 1.6067590713500977 seconds\n",
            "\n",
            " 62 Inference mcocr_public_145014zsdex.png\n",
            "get boxes from icdar time: 0.002156496047973633 seconds\n",
            "Classifier_Vietocr. Inference 29 boxes\n",
            "mean prob: 0.7158296829580657\n",
            "Multiscale OCR time: 0.0 seconds\n",
            "Total predict time: 1.0281713008880615 seconds\n",
            "Save visualized result to /content/gdrive/MyDrive/MC_OCR/Data2/viz_imgs/mcocr_public_145014zsdex.png\n",
            "Visualize time: 2.034122943878174 seconds\n",
            "\n",
            " 63 Inference mcocr_val_145114ixmyt.png\n",
            "get boxes from icdar time: 0.002367258071899414 seconds\n",
            "Classifier_Vietocr. Inference 31 boxes\n",
            "mean prob: 0.6893599965021977\n",
            "\n",
            " 64 Inference mcocr_val_145114lbdpw.png\n",
            "get boxes from icdar time: 0.001977205276489258 seconds\n",
            "Classifier_Vietocr. Inference 22 boxes\n",
            "mean prob: 0.6556908800509488\n",
            "\n",
            " 65 Inference mcocr_val_145115cfwch.png\n",
            "get boxes from icdar time: 0.0026743412017822266 seconds\n",
            "Classifier_Vietocr. Inference 72 boxes\n",
            "mean prob: 0.6997792366323821\n",
            "\n",
            " 66 Inference mcocr_val_145115fkifm.png\n",
            "get boxes from icdar time: 0.0023581981658935547 seconds\n",
            "Classifier_Vietocr. Inference 47 boxes\n",
            "mean prob: 0.6852114378969063\n",
            "\n",
            " 67 Inference mcocr_val_145115itmlf.png\n",
            "get boxes from icdar time: 0.002159595489501953 seconds\n",
            "Classifier_Vietocr. Inference 31 boxes\n",
            "mean prob: 0.7690592525222827\n",
            "Multiscale OCR time: 0.0 seconds\n",
            "Total predict time: 1.1275553703308105 seconds\n",
            "Save visualized result to /content/gdrive/MyDrive/MC_OCR/Data2/viz_imgs/mcocr_val_145115itmlf.png\n",
            "Visualize time: 1.6473016738891602 seconds\n",
            "\n",
            " 68 Inference mcocr_val_145115mxzac.png\n",
            "get boxes from icdar time: 0.004981040954589844 seconds\n",
            "Classifier_Vietocr. Inference 51 boxes\n",
            "mean prob: 0.6533712957226032\n",
            "\n",
            " 69 Inference mcocr_val_145115ndmhc.png\n",
            "get boxes from icdar time: 0.002451658248901367 seconds\n",
            "Classifier_Vietocr. Inference 24 boxes\n",
            "mean prob: 0.8034303725163637\n",
            "Multiscale OCR time: 0.0 seconds\n",
            "Total predict time: 0.8056695461273193 seconds\n",
            "Save visualized result to /content/gdrive/MyDrive/MC_OCR/Data2/viz_imgs/mcocr_val_145115ndmhc.png\n",
            "Visualize time: 1.421870231628418 seconds\n",
            "\n",
            " 70 Inference mcocr_val_145115obsjf.png\n",
            "get boxes from icdar time: 0.002254486083984375 seconds\n",
            "Classifier_Vietocr. Inference 28 boxes\n",
            "mean prob: 0.7149581863146776\n",
            "Multiscale OCR time: 0.0 seconds\n",
            "Total predict time: 1.128586769104004 seconds\n",
            "/content/gdrive/MyDrive/MC_OCR/mc_ocr/utils/visualize.py:22: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "  fig, ax = plt.subplots(1)\n",
            "Save visualized result to /content/gdrive/MyDrive/MC_OCR/Data2/viz_imgs/mcocr_val_145115obsjf.png\n",
            "Visualize time: 1.1809029579162598 seconds\n",
            "\n",
            " 71 Inference mcocr_val_145115rltwx.png\n",
            "get boxes from icdar time: 0.0021266937255859375 seconds\n",
            "Classifier_Vietocr. Inference 30 boxes\n",
            "mean prob: 0.5980767566275017\n",
            "\n",
            " 72 Inference mcocr_val_145115ujvia.png\n",
            "get boxes from icdar time: 0.002428770065307617 seconds\n",
            "Classifier_Vietocr. Inference 48 boxes\n",
            "mean prob: 0.6211894809741157\n",
            "Processing time: 129.70805954933167 seconds. Speed: 1.7768 second/image\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install trdg"
      ],
      "metadata": {
        "id": "1lQIL8wnD1E5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gdrive/MyDrive/MC_OCR/mc_ocr/rotation_corrector\n",
        "!python3 data_process.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9FeqHfUC6xam",
        "outputId": "ea73ef1a-3c5d-4ec4-d484-6dbc971da702"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/MC_OCR/mc_ocr/rotation_corrector\n",
            "Missing modules for handwritten text generation.\n",
            "generated 0 images\n",
            "generated 1000 images\n",
            "generated 2000 images\n",
            "generated 3000 images\n",
            "generated 4000 images\n",
            "generated 5000 images\n",
            "generated 6000 images\n",
            "generated 7000 images\n",
            "generated 8000 images\n",
            "generated 9000 images\n",
            "generated 10000 images\n",
            "generate synthetic images done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install yacs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ny2NAjMEfjub",
        "outputId": "d97e93d8-4040-4e57-e5f3-d4e9dde45afe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting yacs\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from yacs) (3.13)\n",
            "Installing collected packages: yacs\n",
            "Successfully installed yacs-0.1.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gdrive/MyDrive/MC_OCR/mc_ocr/rotation_corrector\n",
        "!python3 train_config.py --cfg experiments/mobilenetv3_filtered_public_train.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wahYMmcQfPGJ",
        "outputId": "e4df0a83-e897-4c3a-f842-eec83112157b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/MC_OCR/mc_ocr/rotation_corrector\n",
            "2022-04-15 04:32:35,378 - INFO - Namespace(cfg='experiments/mobilenetv3_filtered_public_train.yaml')\n",
            "2022-04-15 04:32:35,382 - INFO - AUTO_RESUME: False\n",
            "DATASET:\n",
            "  DATASET: rotation_corrector\n",
            "  NUM_CLASSES: 2\n",
            "  ROOT: \n",
            "  TEST_LIST: /content/gdrive/MyDrive/MC_OCR/data/line_cropped/val.txt\n",
            "  TRAIN_LIST: /content/gdrive/MyDrive/MC_OCR/data/line_cropped/train.txt\n",
            "DEBUG:\n",
            "  DEBUG: False\n",
            "GPUS: 0\n",
            "LOG_DIR: log\n",
            "MODEL:\n",
            "  BACKBONE: \n",
            "  HEAD: \n",
            "  NAME: mobilenetv3\n",
            "  PRETRAINED: \n",
            "OUTPUT_DIR: output\n",
            "TEST:\n",
            "  BATCH_SIZE: 256\n",
            "  CENTER_CROP_TEST: False\n",
            "  IMAGE_SIZE: [64, 192]\n",
            "  MODEL_FILE: \n",
            "TRAIN:\n",
            "  BATCH_SIZE: 256\n",
            "  BEGIN_EPOCH: 0\n",
            "  DROPOUT: 0.2\n",
            "  END_EPOCH: 50\n",
            "  IMAGE_SIZE: [64, 192]\n",
            "  LR: 0.001\n",
            "  OPTIMIZER: adam\n",
            "  PRINT_FREQ: 100\n",
            "  RANDOM_CROP: True\n",
            "  RESIZE: False\n",
            "  RESUME: True\n",
            "  SHUFFLE: True\n",
            "  VALIDATION_EPOCH: 1\n",
            "WORKERS: 4\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:288: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "2\n",
            "2022-04-15 04:32:42,896 - INFO - Epoch [0/50]\n",
            "eval model:: 100% 8/8 [00:07<00:00,  1.08it/s]\n",
            "2022-04-15 04:33:20,096 - INFO - val acc: 0.52, val loss 0.693, fps 3242.955\n",
            "2022-04-15 04:33:20,166 - INFO - Saved model output/rotation_corrector/mobilenetv3/train_20220415_0432/mobilenetv3-Epoch-0-Loss-0.693-Acc-0.52.pth\n",
            "2022-04-15 04:33:20,168 - INFO - best val Acc: 0.52 in epoch: 0\n",
            "2022-04-15 04:33:20,171 - INFO - Min val Loss: 0.693 in epoch: 0\n",
            "2022-04-15 04:33:20,172 - INFO - --------------------------------------------\n",
            "2022-04-15 04:33:20,172 - INFO - Epoch [1/50]\n",
            "eval model:: 100% 8/8 [00:08<00:00,  1.02s/it]\n",
            "2022-04-15 04:33:58,020 - INFO - val acc: 0.53, val loss 0.69, fps 2973.497\n",
            "2022-04-15 04:33:58,086 - INFO - Saved model output/rotation_corrector/mobilenetv3/train_20220415_0432/mobilenetv3-Epoch-1-Loss-0.69-Acc-0.53.pth\n",
            "2022-04-15 04:33:58,086 - INFO - best val Acc: 0.53 in epoch: 1\n",
            "2022-04-15 04:33:58,086 - INFO - Min val Loss: 0.69 in epoch: 1\n",
            "2022-04-15 04:33:58,087 - INFO - --------------------------------------------\n",
            "2022-04-15 04:33:58,087 - INFO - Epoch [2/50]\n",
            "eval model:: 100% 8/8 [00:07<00:00,  1.08it/s]\n",
            "2022-04-15 04:34:35,236 - INFO - val acc: 0.6, val loss 0.681, fps 2689.647\n",
            "2022-04-15 04:34:35,309 - INFO - Saved model output/rotation_corrector/mobilenetv3/train_20220415_0432/mobilenetv3-Epoch-2-Loss-0.681-Acc-0.6.pth\n",
            "2022-04-15 04:34:35,309 - INFO - best val Acc: 0.6 in epoch: 2\n",
            "2022-04-15 04:34:35,309 - INFO - Min val Loss: 0.681 in epoch: 2\n",
            "2022-04-15 04:34:35,310 - INFO - --------------------------------------------\n",
            "2022-04-15 04:34:35,310 - INFO - Epoch [3/50]\n",
            "eval model:: 100% 8/8 [00:07<00:00,  1.09it/s]\n",
            "2022-04-15 04:35:11,983 - INFO - val acc: 0.62, val loss 0.664, fps 3172.612\n",
            "2022-04-15 04:35:12,087 - INFO - Saved model output/rotation_corrector/mobilenetv3/train_20220415_0432/mobilenetv3-Epoch-3-Loss-0.664-Acc-0.62.pth\n",
            "2022-04-15 04:35:12,088 - INFO - best val Acc: 0.62 in epoch: 3\n",
            "2022-04-15 04:35:12,088 - INFO - Min val Loss: 0.664 in epoch: 3\n",
            "2022-04-15 04:35:12,088 - INFO - --------------------------------------------\n",
            "2022-04-15 04:35:12,089 - INFO - Epoch [4/50]\n",
            "eval model:: 100% 8/8 [00:07<00:00,  1.09it/s]\n",
            "2022-04-15 04:35:48,702 - INFO - val acc: 0.69, val loss 0.59, fps 2909.625\n",
            "2022-04-15 04:35:48,774 - INFO - Saved model output/rotation_corrector/mobilenetv3/train_20220415_0432/mobilenetv3-Epoch-4-Loss-0.59-Acc-0.69.pth\n",
            "2022-04-15 04:35:48,774 - INFO - best val Acc: 0.69 in epoch: 4\n",
            "2022-04-15 04:35:48,774 - INFO - Min val Loss: 0.59 in epoch: 4\n",
            "2022-04-15 04:35:48,775 - INFO - --------------------------------------------\n",
            "2022-04-15 04:35:48,775 - INFO - Epoch [5/50]\n",
            "eval model:: 100% 8/8 [00:07<00:00,  1.10it/s]\n",
            "2022-04-15 04:36:25,585 - INFO - val acc: 0.76, val loss 0.501, fps 3294.357\n",
            "2022-04-15 04:36:25,649 - INFO - Saved model output/rotation_corrector/mobilenetv3/train_20220415_0432/mobilenetv3-Epoch-5-Loss-0.501-Acc-0.76.pth\n",
            "2022-04-15 04:36:25,650 - INFO - best val Acc: 0.76 in epoch: 5\n",
            "2022-04-15 04:36:25,651 - INFO - Min val Loss: 0.501 in epoch: 5\n",
            "2022-04-15 04:36:25,651 - INFO - --------------------------------------------\n",
            "2022-04-15 04:36:25,651 - INFO - Epoch [6/50]\n",
            "eval model:: 100% 8/8 [00:07<00:00,  1.07it/s]\n",
            "2022-04-15 04:37:02,818 - INFO - val acc: 0.76, val loss 0.497, fps 3281.638\n",
            "2022-04-15 04:37:02,903 - INFO - Saved model output/rotation_corrector/mobilenetv3/train_20220415_0432/mobilenetv3-Epoch-6-Loss-0.497-Acc-0.76.pth\n",
            "2022-04-15 04:37:02,903 - INFO - best val Acc: 0.76 in epoch: 5\n",
            "2022-04-15 04:37:02,903 - INFO - Min val Loss: 0.497 in epoch: 6\n",
            "2022-04-15 04:37:02,904 - INFO - --------------------------------------------\n",
            "2022-04-15 04:37:02,904 - INFO - Epoch [7/50]\n",
            "eval model:: 100% 8/8 [00:07<00:00,  1.08it/s]\n",
            "2022-04-15 04:37:39,923 - INFO - val acc: 0.77, val loss 0.441, fps 3262.175\n",
            "2022-04-15 04:37:39,992 - INFO - Saved model output/rotation_corrector/mobilenetv3/train_20220415_0432/mobilenetv3-Epoch-7-Loss-0.441-Acc-0.77.pth\n",
            "2022-04-15 04:37:39,994 - INFO - best val Acc: 0.77 in epoch: 7\n",
            "2022-04-15 04:37:39,994 - INFO - Min val Loss: 0.441 in epoch: 7\n",
            "2022-04-15 04:37:39,995 - INFO - --------------------------------------------\n",
            "2022-04-15 04:37:39,995 - INFO - Epoch [8/50]\n",
            "eval model:: 100% 8/8 [00:07<00:00,  1.09it/s]\n",
            "2022-04-15 04:38:17,260 - INFO - val acc: 0.82, val loss 0.386, fps 3221.97\n",
            "2022-04-15 04:38:17,353 - INFO - Saved model output/rotation_corrector/mobilenetv3/train_20220415_0432/mobilenetv3-Epoch-8-Loss-0.386-Acc-0.82.pth\n",
            "2022-04-15 04:38:17,353 - INFO - best val Acc: 0.82 in epoch: 8\n",
            "2022-04-15 04:38:17,353 - INFO - Min val Loss: 0.386 in epoch: 8\n",
            "2022-04-15 04:38:17,354 - INFO - --------------------------------------------\n",
            "2022-04-15 04:38:17,354 - INFO - Epoch [9/50]\n",
            "eval model:: 100% 8/8 [00:07<00:00,  1.10it/s]\n",
            "2022-04-15 04:38:54,077 - INFO - val acc: 0.82, val loss 0.354, fps 3191.618\n",
            "2022-04-15 04:38:54,141 - INFO - Saved model output/rotation_corrector/mobilenetv3/train_20220415_0432/mobilenetv3-Epoch-9-Loss-0.354-Acc-0.82.pth\n",
            "2022-04-15 04:38:54,143 - INFO - best val Acc: 0.82 in epoch: 8\n",
            "2022-04-15 04:38:54,143 - INFO - Min val Loss: 0.354 in epoch: 9\n",
            "2022-04-15 04:38:54,143 - INFO - --------------------------------------------\n",
            "2022-04-15 04:38:54,144 - INFO - Epoch [10/50]\n",
            "eval model:: 100% 8/8 [00:07<00:00,  1.10it/s]\n",
            "2022-04-15 04:39:30,918 - INFO - val acc: 0.82, val loss 0.348, fps 3250.136\n",
            "2022-04-15 04:39:30,991 - INFO - Saved model output/rotation_corrector/mobilenetv3/train_20220415_0432/mobilenetv3-Epoch-10-Loss-0.348-Acc-0.82.pth\n",
            "2022-04-15 04:39:30,991 - INFO - best val Acc: 0.82 in epoch: 8\n",
            "2022-04-15 04:39:30,991 - INFO - Min val Loss: 0.348 in epoch: 10\n",
            "2022-04-15 04:39:30,991 - INFO - --------------------------------------------\n",
            "2022-04-15 04:39:30,992 - INFO - Epoch [11/50]\n",
            "eval model:: 100% 8/8 [00:07<00:00,  1.07it/s]\n",
            "2022-04-15 04:40:07,531 - INFO - val acc: 0.87, val loss 0.29, fps 3173.642\n",
            "2022-04-15 04:40:07,600 - INFO - Saved model output/rotation_corrector/mobilenetv3/train_20220415_0432/mobilenetv3-Epoch-11-Loss-0.29-Acc-0.87.pth\n",
            "2022-04-15 04:40:07,601 - INFO - best val Acc: 0.87 in epoch: 11\n",
            "2022-04-15 04:40:07,602 - INFO - Min val Loss: 0.29 in epoch: 11\n",
            "2022-04-15 04:40:07,602 - INFO - --------------------------------------------\n",
            "2022-04-15 04:40:07,602 - INFO - Epoch [12/50]\n",
            "eval model:: 100% 8/8 [00:07<00:00,  1.09it/s]\n",
            "2022-04-15 04:40:44,164 - INFO - val acc: 0.87, val loss 0.297, fps 2881.106\n",
            "2022-04-15 04:40:44,172 - INFO - best val Acc: 0.87 in epoch: 11\n",
            "2022-04-15 04:40:44,172 - INFO - Min val Loss: 0.29 in epoch: 11\n",
            "2022-04-15 04:40:44,173 - INFO - --------------------------------------------\n",
            "2022-04-15 04:40:44,173 - INFO - Epoch [13/50]\n",
            "eval model:: 100% 8/8 [00:07<00:00,  1.09it/s]\n",
            "2022-04-15 04:41:21,159 - INFO - val acc: 0.87, val loss 0.277, fps 3202.983\n",
            "2022-04-15 04:41:21,230 - INFO - Saved model output/rotation_corrector/mobilenetv3/train_20220415_0432/mobilenetv3-Epoch-13-Loss-0.277-Acc-0.87.pth\n",
            "2022-04-15 04:41:21,231 - INFO - best val Acc: 0.87 in epoch: 11\n",
            "2022-04-15 04:41:21,232 - INFO - Min val Loss: 0.277 in epoch: 13\n",
            "2022-04-15 04:41:21,232 - INFO - --------------------------------------------\n",
            "2022-04-15 04:41:21,232 - INFO - Epoch [14/50]\n",
            "eval model:: 100% 8/8 [00:07<00:00,  1.08it/s]\n",
            "2022-04-15 04:41:58,164 - INFO - val acc: 0.87, val loss 0.256, fps 3191.766\n",
            "2022-04-15 04:41:58,236 - INFO - Saved model output/rotation_corrector/mobilenetv3/train_20220415_0432/mobilenetv3-Epoch-14-Loss-0.256-Acc-0.87.pth\n",
            "2022-04-15 04:41:58,237 - INFO - best val Acc: 0.87 in epoch: 11\n",
            "2022-04-15 04:41:58,237 - INFO - Min val Loss: 0.256 in epoch: 14\n",
            "2022-04-15 04:41:58,237 - INFO - --------------------------------------------\n",
            "2022-04-15 04:41:58,238 - INFO - Epoch [15/50]\n",
            "eval model:: 100% 8/8 [00:07<00:00,  1.08it/s]\n",
            "2022-04-15 04:42:35,881 - INFO - val acc: 0.9, val loss 0.218, fps 3240.648\n",
            "2022-04-15 04:42:35,949 - INFO - Saved model output/rotation_corrector/mobilenetv3/train_20220415_0432/mobilenetv3-Epoch-15-Loss-0.218-Acc-0.9.pth\n",
            "2022-04-15 04:42:35,949 - INFO - best val Acc: 0.9 in epoch: 15\n",
            "2022-04-15 04:42:35,950 - INFO - Min val Loss: 0.218 in epoch: 15\n",
            "2022-04-15 04:42:35,950 - INFO - --------------------------------------------\n",
            "2022-04-15 04:42:35,950 - INFO - Epoch [16/50]\n",
            "eval model:: 100% 8/8 [00:07<00:00,  1.08it/s]\n",
            "2022-04-15 04:43:12,895 - INFO - val acc: 0.9, val loss 0.217, fps 2749.004\n",
            "2022-04-15 04:43:12,976 - INFO - Saved model output/rotation_corrector/mobilenetv3/train_20220415_0432/mobilenetv3-Epoch-16-Loss-0.217-Acc-0.9.pth\n",
            "2022-04-15 04:43:12,977 - INFO - best val Acc: 0.9 in epoch: 15\n",
            "2022-04-15 04:43:12,978 - INFO - Min val Loss: 0.217 in epoch: 16\n",
            "2022-04-15 04:43:12,978 - INFO - --------------------------------------------\n",
            "2022-04-15 04:43:12,979 - INFO - Epoch [17/50]\n",
            "eval model:: 100% 8/8 [00:07<00:00,  1.11it/s]\n",
            "2022-04-15 04:43:49,858 - INFO - val acc: 0.86, val loss 0.316, fps 3234.833\n",
            "2022-04-15 04:43:49,863 - INFO - best val Acc: 0.9 in epoch: 15\n",
            "2022-04-15 04:43:49,863 - INFO - Min val Loss: 0.217 in epoch: 16\n",
            "2022-04-15 04:43:49,864 - INFO - --------------------------------------------\n",
            "2022-04-15 04:43:49,864 - INFO - Epoch [18/50]\n",
            "eval model:: 100% 8/8 [00:07<00:00,  1.10it/s]\n",
            "2022-04-15 04:44:26,704 - INFO - val acc: 0.88, val loss 0.279, fps 3251.026\n",
            "2022-04-15 04:44:26,709 - INFO - best val Acc: 0.9 in epoch: 15\n",
            "2022-04-15 04:44:26,709 - INFO - Min val Loss: 0.217 in epoch: 16\n",
            "2022-04-15 04:44:26,709 - INFO - --------------------------------------------\n",
            "2022-04-15 04:44:26,710 - INFO - Epoch [19/50]\n",
            "eval model:: 100% 8/8 [00:07<00:00,  1.12it/s]\n",
            "2022-04-15 04:45:02,783 - INFO - val acc: 0.91, val loss 0.214, fps 3273.51\n",
            "2022-04-15 04:45:02,848 - INFO - Saved model output/rotation_corrector/mobilenetv3/train_20220415_0432/mobilenetv3-Epoch-19-Loss-0.214-Acc-0.91.pth\n",
            "2022-04-15 04:45:02,849 - INFO - best val Acc: 0.91 in epoch: 19\n",
            "2022-04-15 04:45:02,850 - INFO - Min val Loss: 0.214 in epoch: 19\n",
            "2022-04-15 04:45:02,850 - INFO - --------------------------------------------\n",
            "2022-04-15 04:45:02,850 - INFO - Epoch [20/50]\n",
            "eval model:: 100% 8/8 [00:07<00:00,  1.10it/s]\n",
            "2022-04-15 04:45:38,913 - INFO - val acc: 0.91, val loss 0.209, fps 2679.829\n",
            "2022-04-15 04:45:38,975 - INFO - Saved model output/rotation_corrector/mobilenetv3/train_20220415_0432/mobilenetv3-Epoch-20-Loss-0.209-Acc-0.91.pth\n",
            "2022-04-15 04:45:38,976 - INFO - best val Acc: 0.91 in epoch: 19\n",
            "2022-04-15 04:45:38,976 - INFO - Min val Loss: 0.209 in epoch: 20\n",
            "2022-04-15 04:45:38,976 - INFO - --------------------------------------------\n",
            "2022-04-15 04:45:38,976 - INFO - Epoch [21/50]\n",
            "eval model:: 100% 8/8 [00:07<00:00,  1.12it/s]\n",
            "2022-04-15 04:46:14,991 - INFO - val acc: 0.92, val loss 0.182, fps 3288.393\n",
            "2022-04-15 04:46:15,058 - INFO - Saved model output/rotation_corrector/mobilenetv3/train_20220415_0432/mobilenetv3-Epoch-21-Loss-0.182-Acc-0.92.pth\n",
            "2022-04-15 04:46:15,059 - INFO - best val Acc: 0.92 in epoch: 21\n",
            "2022-04-15 04:46:15,061 - INFO - Min val Loss: 0.182 in epoch: 21\n",
            "2022-04-15 04:46:15,061 - INFO - --------------------------------------------\n",
            "2022-04-15 04:46:15,062 - INFO - Epoch [22/50]\n",
            "eval model:: 100% 8/8 [00:07<00:00,  1.10it/s]\n",
            "2022-04-15 04:46:51,029 - INFO - val acc: 0.92, val loss 0.196, fps 2697.591\n",
            "2022-04-15 04:46:51,034 - INFO - best val Acc: 0.92 in epoch: 21\n",
            "2022-04-15 04:46:51,034 - INFO - Min val Loss: 0.182 in epoch: 21\n",
            "2022-04-15 04:46:51,034 - INFO - --------------------------------------------\n",
            "2022-04-15 04:46:51,034 - INFO - Epoch [23/50]\n",
            "eval model:: 100% 8/8 [00:07<00:00,  1.12it/s]\n",
            "2022-04-15 04:47:26,911 - INFO - val acc: 0.92, val loss 0.17, fps 3257.516\n",
            "2022-04-15 04:47:26,979 - INFO - Saved model output/rotation_corrector/mobilenetv3/train_20220415_0432/mobilenetv3-Epoch-23-Loss-0.17-Acc-0.92.pth\n",
            "2022-04-15 04:47:26,980 - INFO - best val Acc: 0.92 in epoch: 21\n",
            "2022-04-15 04:47:26,981 - INFO - Min val Loss: 0.17 in epoch: 23\n",
            "2022-04-15 04:47:26,981 - INFO - --------------------------------------------\n",
            "2022-04-15 04:47:26,981 - INFO - Epoch [24/50]\n",
            "eval model:: 100% 8/8 [00:07<00:00,  1.12it/s]\n",
            "2022-04-15 04:48:02,960 - INFO - val acc: 0.91, val loss 0.217, fps 3121.022\n",
            "2022-04-15 04:48:02,965 - INFO - best val Acc: 0.92 in epoch: 21\n",
            "2022-04-15 04:48:02,965 - INFO - Min val Loss: 0.17 in epoch: 23\n",
            "2022-04-15 04:48:02,965 - INFO - --------------------------------------------\n",
            "2022-04-15 04:48:02,965 - INFO - Epoch [25/50]\n",
            "eval model:: 100% 8/8 [00:07<00:00,  1.13it/s]\n",
            "2022-04-15 04:48:38,820 - INFO - val acc: 0.92, val loss 0.176, fps 2591.388\n",
            "2022-04-15 04:48:38,825 - INFO - best val Acc: 0.92 in epoch: 21\n",
            "2022-04-15 04:48:38,825 - INFO - Min val Loss: 0.17 in epoch: 23\n",
            "2022-04-15 04:48:38,826 - INFO - --------------------------------------------\n",
            "2022-04-15 04:48:38,826 - INFO - Epoch [26/50]\n",
            "eval model:: 100% 8/8 [00:07<00:00,  1.11it/s]\n",
            "2022-04-15 04:49:14,589 - INFO - val acc: 0.93, val loss 0.158, fps 3153.687\n",
            "2022-04-15 04:49:14,656 - INFO - Saved model output/rotation_corrector/mobilenetv3/train_20220415_0432/mobilenetv3-Epoch-26-Loss-0.158-Acc-0.93.pth\n",
            "2022-04-15 04:49:14,656 - INFO - best val Acc: 0.93 in epoch: 26\n",
            "2022-04-15 04:49:14,656 - INFO - Min val Loss: 0.158 in epoch: 26\n",
            "2022-04-15 04:49:14,657 - INFO - --------------------------------------------\n",
            "2022-04-15 04:49:14,657 - INFO - Epoch [27/50]\n",
            "eval model:: 100% 8/8 [00:07<00:00,  1.13it/s]\n",
            "2022-04-15 04:49:50,094 - INFO - val acc: 0.92, val loss 0.196, fps 3215.202\n",
            "2022-04-15 04:49:50,100 - INFO - best val Acc: 0.93 in epoch: 26\n",
            "2022-04-15 04:49:50,101 - INFO - Min val Loss: 0.158 in epoch: 26\n",
            "2022-04-15 04:49:50,101 - INFO - --------------------------------------------\n",
            "2022-04-15 04:49:50,101 - INFO - Epoch [28/50]\n",
            "eval model:: 100% 8/8 [00:07<00:00,  1.12it/s]\n",
            "2022-04-15 04:50:25,503 - INFO - val acc: 0.9, val loss 0.225, fps 2478.128\n",
            "2022-04-15 04:50:25,508 - INFO - best val Acc: 0.93 in epoch: 26\n",
            "2022-04-15 04:50:25,508 - INFO - Min val Loss: 0.158 in epoch: 26\n",
            "2022-04-15 04:50:25,508 - INFO - --------------------------------------------\n",
            "2022-04-15 04:50:25,508 - INFO - Epoch [29/50]\n",
            "eval model:: 100% 8/8 [00:07<00:00,  1.11it/s]\n",
            "2022-04-15 04:51:01,547 - INFO - val acc: 0.93, val loss 0.185, fps 3143.396\n",
            "2022-04-15 04:51:01,553 - INFO - best val Acc: 0.93 in epoch: 26\n",
            "2022-04-15 04:51:01,553 - INFO - Min val Loss: 0.158 in epoch: 26\n",
            "2022-04-15 04:51:01,553 - INFO - --------------------------------------------\n",
            "2022-04-15 04:51:01,554 - INFO - Epoch [30/50]\n",
            "eval model:: 100% 8/8 [00:07<00:00,  1.11it/s]\n",
            "2022-04-15 04:51:38,212 - INFO - val acc: 0.93, val loss 0.151, fps 3292.498\n",
            "2022-04-15 04:51:38,277 - INFO - Saved model output/rotation_corrector/mobilenetv3/train_20220415_0432/mobilenetv3-Epoch-30-Loss-0.151-Acc-0.93.pth\n",
            "2022-04-15 04:51:38,278 - INFO - best val Acc: 0.93 in epoch: 26\n",
            "2022-04-15 04:51:38,278 - INFO - Min val Loss: 0.151 in epoch: 30\n",
            "2022-04-15 04:51:38,279 - INFO - --------------------------------------------\n",
            "2022-04-15 04:51:38,279 - INFO - Epoch [31/50]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1hlBhM9xfTOT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}